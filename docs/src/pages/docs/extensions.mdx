---
title: Extensions Overview
description: Learn about Jan's default extensions and explore how to configure them.
  [
    Jan,
    Customizable Intelligence, LLM,
    local AI,
    privacy focus,
    free and open source,
    private and offline,
    conversational AI,
    no-subscription fee,
    large language models,
    Jan Extensions,
    Extensions,
  ]
---

import { Callout } from 'nextra/components'
import { Settings, EllipsisVertical } from 'lucide-react'


# Extensions 

## Overview
Extensions are modular components that add functionality to Jan. Each extension is designed to handle specific features. 

Extensions can be managed through **Settings** (<Settings width={16} height={16} style={{display:"inline"}}/>) > **Extensions**:

<<<<<<< HEAD
## List of Core Extensions
| Extension Name      | Version  | Description                                                                 |
|---------------------|----------|-----------------------------------------------------------------------------|
| Jan Assistant       | v1.0.1   | Powers the default AI assistant that works with all your installed models. |
| Conversational      | v1.0.0   | Enables conversations and state persistence via your filesystem. |
| Model Management    | v1.0.33  | Model Management Extension provides model exploration and seamless downloads |
| System Monitoring   | v1.0.10  | Provides system health and OS level data.                     |


## Configure an Extension Settings

To configure an extension settings:

1. Navigate to the `~/jan/data/extensions`.
2. Open the `extensions.json` file
3. Edit the file with options including:

| Option           | Description                         |
| ---------------- | ----------------------------------- |
| `_active`        | Enable/disable the extension.       |
| `listeners`      | Default listener setting.           |
| `origin`         | Extension file path.                |
| `installOptions` | Version and metadata configuration. |
| `name`           | Extension name.                     |
| `productName`    | Extension display name.                     |
| `version`        | Extension version.                  |
| `main`           | Main file path.                     |
| `description`    | Extension description.              |
| `url`            | Extension URL.                      |

```json title="~/jan/data/extensions/extensions.json"
{
    "@janhq/conversational-extension": {
        "_active": true,
        "listeners": {},
        "origin": "C:\\Users\\ACER\\AppData\\Local\\Programs\\jan\\resources\\app.asar.unpacked\\pre-install\\janhq-conversational-extension-1.0.0.tgz",
        "installOptions": {
            "version": false,
            "fullMetadata": true
        },
        "name": "@janhq/conversational-extension",
        "productName": "Conversational",
        "version": "1.0.0",
        "main": "dist/index.js",
        "description": "Enables conversations and state persistence via your filesystem.",
        "url": "extension://@janhq/conversational-extension/dist/index.js"
    },
    "@janhq/inference-anthropic-extension": {
        "_active": true,
        "listeners": {},
        "origin": "C:\\Users\\ACER\\AppData\\Local\\Programs\\jan\\resources\\app.asar.unpacked\\pre-install\\janhq-inference-anthropic-extension-1.0.2.tgz",
        "installOptions": {
            "version": false,
            "fullMetadata": true
        },
        "name": "@janhq/inference-anthropic-extension",
        "productName": "Anthropic Inference Engine",
        "version": "1.0.2",
        "main": "dist/index.js",
        "description": "This extension enables Anthropic chat completion API calls",
        "url": "extension://@janhq/inference-anthropic-extension/dist/index.js"
    },
    "@janhq/inference-triton-trt-llm-extension": {
        "_active": true,
        "listeners": {},
        "origin": "C:\\Users\\ACER\\AppData\\Local\\Programs\\jan\\resources\\app.asar.unpacked\\pre-install\\janhq-inference-triton-trt-llm-extension-1.0.0.tgz",
        "installOptions": {
            "version": false,
            "fullMetadata": true
        },
        "name": "@janhq/inference-triton-trt-llm-extension",
        "productName": "Triton-TRT-LLM Inference Engine",
        "version": "1.0.0",
        "main": "dist/index.js",
        "description": "This extension enables Nvidia's TensorRT-LLM as an inference engine option",
        "url": "extension://@janhq/inference-triton-trt-llm-extension/dist/index.js"
    },
    "@janhq/inference-mistral-extension": {
        "_active": true,
        "listeners": {},
        "origin": "C:\\Users\\ACER\\AppData\\Local\\Programs\\jan\\resources\\app.asar.unpacked\\pre-install\\janhq-inference-mistral-extension-1.0.1.tgz",
        "installOptions": {
            "version": false,
            "fullMetadata": true
        },
        "name": "@janhq/inference-mistral-extension",
        "productName": "MistralAI Inference Engine",
        "version": "1.0.1",
        "main": "dist/index.js",
        "description": "This extension enables Mistral chat completion API calls",
        "url": "extension://@janhq/inference-mistral-extension/dist/index.js"
    },
    "@janhq/inference-martian-extension": {
        "_active": true,
        "listeners": {},
        "origin": "C:\\Users\\ACER\\AppData\\Local\\Programs\\jan\\resources\\app.asar.unpacked\\pre-install\\janhq-inference-martian-extension-1.0.1.tgz",
        "installOptions": {
            "version": false,
            "fullMetadata": true
        },
        "name": "@janhq/inference-martian-extension",
        "productName": "Martian Inference Engine",
        "version": "1.0.1",
        "main": "dist/index.js",
        "description": "This extension enables Martian chat completion API calls",
        "url": "extension://@janhq/inference-martian-extension/dist/index.js"
    },
    "@janhq/inference-openrouter-extension": {
        "_active": true,
        "listeners": {},
        "origin": "C:\\Users\\ACER\\AppData\\Local\\Programs\\jan\\resources\\app.asar.unpacked\\pre-install\\janhq-inference-openrouter-extension-1.0.0.tgz",
        "installOptions": {
            "version": false,
            "fullMetadata": true
        },
        "name": "@janhq/inference-openrouter-extension",
        "productName": "OpenRouter Inference Engine",
        "version": "1.0.0",
        "main": "dist/index.js",
        "description": "This extension enables Open Router chat completion API calls",
        "url": "extension://@janhq/inference-openrouter-extension/dist/index.js"
    },
    "@janhq/inference-nvidia-extension": {
        "_active": true,
        "listeners": {},
        "origin": "C:\\Users\\ACER\\AppData\\Local\\Programs\\jan\\resources\\app.asar.unpacked\\pre-install\\janhq-inference-nvidia-extension-1.0.1.tgz",
        "installOptions": {
            "version": false,
            "fullMetadata": true
        },
        "name": "@janhq/inference-nvidia-extension",
        "productName": "NVIDIA NIM Inference Engine",
        "version": "1.0.1",
        "main": "dist/index.js",
        "description": "This extension enables NVIDIA chat completion API calls",
        "url": "extension://@janhq/inference-nvidia-extension/dist/index.js"
    },
    "@janhq/inference-groq-extension": {
        "_active": true,
        "listeners": {},
        "origin": "C:\\Users\\ACER\\AppData\\Local\\Programs\\jan\\resources\\app.asar.unpacked\\pre-install\\janhq-inference-groq-extension-1.0.1.tgz",
        "installOptions": {
            "version": false,
            "fullMetadata": true
        },
        "name": "@janhq/inference-groq-extension",
        "productName": "Groq Inference Engine",
        "version": "1.0.1",
        "main": "dist/index.js",
        "description": "This extension enables fast Groq chat completion API calls",
        "url": "extension://@janhq/inference-groq-extension/dist/index.js"
    },
    "@janhq/inference-openai-extension": {
        "_active": true,
        "listeners": {},
        "origin": "C:\\Users\\ACER\\AppData\\Local\\Programs\\jan\\resources\\app.asar.unpacked\\pre-install\\janhq-inference-openai-extension-1.0.2.tgz",
        "installOptions": {
            "version": false,
            "fullMetadata": true
        },
        "name": "@janhq/inference-openai-extension",
        "productName": "OpenAI Inference Engine",
        "version": "1.0.2",
        "main": "dist/index.js",
        "description": "This extension enables OpenAI chat completion API calls",
        "url": "extension://@janhq/inference-openai-extension/dist/index.js"
    },
    "@janhq/inference-cohere-extension": {
        "_active": true,
        "listeners": {},
        "origin": "C:\\Users\\ACER\\AppData\\Local\\Programs\\jan\\resources\\app.asar.unpacked\\pre-install\\janhq-inference-cohere-extension-1.0.0.tgz",
        "installOptions": {
            "version": false,
            "fullMetadata": true
        },
        "name": "@janhq/inference-cohere-extension",
        "productName": "Cohere Inference Engine",
        "version": "1.0.0",
        "main": "dist/index.js",
        "description": "This extension enables Cohere chat completion API calls",
        "url": "extension://@janhq/inference-cohere-extension/dist/index.js"
    },
    "@janhq/model-extension": {
        "_active": true,
        "listeners": {},
        "origin": "C:\\Users\\ACER\\AppData\\Local\\Programs\\jan\\resources\\app.asar.unpacked\\pre-install\\janhq-model-extension-1.0.33.tgz",
        "installOptions": {
            "version": false,
            "fullMetadata": true
        },
        "name": "@janhq/model-extension",
        "productName": "Model Management",
        "version": "1.0.33",
        "main": "dist/index.js",
        "description": "Model Management Extension provides model exploration and seamless downloads",
        "url": "extension://@janhq/model-extension/dist/index.js"
    },
    "@janhq/monitoring-extension": {
        "_active": true,
        "listeners": {},
        "origin": "C:\\Users\\ACER\\AppData\\Local\\Programs\\jan\\resources\\app.asar.unpacked\\pre-install\\janhq-monitoring-extension-1.0.10.tgz",
        "installOptions": {
            "version": false,
            "fullMetadata": true
        },
        "name": "@janhq/monitoring-extension",
        "productName": "System Monitoring",
        "version": "1.0.10",
        "main": "dist/index.js",
        "description": "Provides system health and OS level data.",
        "url": "extension://@janhq/monitoring-extension/dist/index.js"
    },
    "@janhq/assistant-extension": {
        "_active": true,
        "listeners": {},
        "origin": "C:\\Users\\ACER\\AppData\\Local\\Programs\\jan\\resources\\app.asar.unpacked\\pre-install\\janhq-assistant-extension-1.0.1.tgz",
        "installOptions": {
            "version": false,
            "fullMetadata": true
        },
        "name": "@janhq/assistant-extension",
        "productName": "Jan Assistant",
        "version": "1.0.1",
        "main": "dist/index.js",
        "description": "Powers the default AI assistant that works with all your installed models.",
        "url": "extension://@janhq/assistant-extension/dist/index.js"
    },
    "@janhq/tensorrt-llm-extension": {
        "_active": true,
        "listeners": {},
        "origin": "C:\\Users\\ACER\\AppData\\Local\\Programs\\jan\\resources\\app.asar.unpacked\\pre-install\\janhq-tensorrt-llm-extension-0.0.3.tgz",
        "installOptions": {
            "version": false,
            "fullMetadata": true
        },
        "name": "@janhq/tensorrt-llm-extension",
        "productName": "TensorRT-LLM Inference Engine",
        "version": "0.0.3",
        "main": "dist/index.js",
        "description": "This extension enables Nvidia's TensorRT-LLM for the fastest GPU acceleration. See the [setup guide](https://jan.ai/guides/providers/tensorrt-llm/) for next steps.",
        "url": "extension://@janhq/tensorrt-llm-extension/dist/index.js"
    },
    "@janhq/inference-cortex-extension": {
        "_active": true,
        "listeners": {},
        "origin": "C:\\Users\\ACER\\AppData\\Local\\Programs\\jan\\resources\\app.asar.unpacked\\pre-install\\janhq-inference-cortex-extension-1.0.15.tgz",
        "installOptions": {
            "version": false,
            "fullMetadata": true
        },
        "name": "@janhq/inference-cortex-extension",
        "productName": "Cortex Inference Engine",
        "version": "1.0.15",
        "main": "dist/index.js",
        "description": "This extension embeds cortex.cpp, a lightweight inference engine written in C++. See https://nitro.jan.ai.\nAdditional dependencies could be installed to run without Cuda Toolkit installation.",
        "url": "extension://@janhq/inference-cortex-extension/dist/index.js"
    }
}
```

## Specific Extension Settings
Jan offers an Extensions settings menu for configuring extensions that have registered their settings within the application. Here, you can directly integrate Remote Inference Engines with Jan without inserting the URL and API Key directly in the `JSON` file. Additionally, you can turn the Logging extensions available on or off in Jan. To access the Extension settings, follow the steps below:
1. Navigate to the main dashboard.
2. Click the **gear icon (⚙️)** on the bottom left of your screen.
=======
>>>>>>> e5b6901b57c5b7edcd5895316f84d742a2772202
<br/>
![Remove Model](./_assets/extensions-01.png)
<br/>

## Core Extensions 

### Cortex
The primary extension that manages both **local** and **remote engines** capabilities:

#### Local Engines
[llama.cpp](/docs/local-engines/llama-cpp): Fast, efficient local inference engine that runs GGUF models directly on your device. Powers Jan's default local AI capabilities with support for multiple hardware configurations.

#### Remote Engines
- [Anthropic](/docs/remote-models/anthropic): Access Claude models
- [Cohere](/docs/remote-models/cohere): Access Cohere's models
- [Groq](/docs/remote-models/groq): High-performance inference
- [Martian](/docs/remote-models/martian): Specialized model access
- [MistralAI](/docs/remote-models/mistralai): Access Mistral models
- [NVIDIA NIM](/docs/remote-models/nvidia-nim) (NVIDIA Inference Microservices): Platform for deploying and serving GPU-accelerated AI models, providing enterprise-grade reliability and scalability.
- [OpenAI](/docs/remote-models/openai): Access GPT models
- [OpenRouter](/docs/remote-models/openrouter): Multi-provider model access
- [Triton-TRT-LLM](/docs/remote-models/triton): High-performance inference backend using NVIDIA Triton Inference Server with TensorRT-LLM optimization, designed for large-scale model deployment.



### Jan Assistant
Enables assistants functionality, including Jan - the default assistant that can utilize all downloaded models. This extension manages:
- Default assistant configurations
- Model selection
- Conversation management

### Conversational 
Manages all chat-related functionality and data persistence:


### [Model Management](/docs/extensions-settings/model-management)
Provides model exploration and seamless downloads:
- Model discovery and browsing
- Version control & configuration handling
- Download management

### [System Monitoring](/docs/extensions-settings/system-monitoring)
Provides system health and OS level data:
- Hardware utilization tracking
- Performance monitoring
- Error logging
